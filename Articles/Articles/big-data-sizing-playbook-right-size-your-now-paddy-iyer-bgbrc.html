<html>
<head>
  <title>The Big Data Sizing Playbook: Right-Size Your Big Data Infrastructure Now</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaD5612AQGzk0uK0ZCNBA" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/big-data-sizing-playbook-right-size-your-now-paddy-iyer-bgbrc">The Big Data Sizing Playbook: Right-Size Your Big Data Infrastructure Now</a></h1>
    <p class="created">Created on 2024-03-02 21:19</p>
  <p class="published">Published on 2024-03-02 22:10</p>
  <div><h3>A Comprehensive Big Data Sizing Guide:</h3><blockquote><p><strong>Disclaimer:</strong> This article provides a starting point for big data sizing on AWS. Remember to experiment with different configurations and monitor your actual usage to optimize your infrastructure for cost and performance. I am not responsible for issues emanating from this</p></blockquote><p>Big Data empowers organizations with incredible opportunities, but managing it effectively hinges on accurately sizing various architectural components. This guide offers essential formulas and considerations for sizing your Big Data infrastructure across different scales:</p><ul><li><p><strong>Small:</strong> Up to a few terabytes (TB)</p></li><li><p><strong>Medium:</strong> 10 TB - 100 TB</p></li><li><p><strong>Large:</strong> 100 TB - 1 petabyte (PB)</p></li><li><p><strong>Extra Large:</strong> Greater than 1 PB</p></li></ul><p>These classifications are just a starting point, and the specific size thresholds can vary depending on the specific context and capabilities of the organization. Here are some additional factors to consider when determining the "right size" for your big data environment:</p><ul><li><p><strong>Data Variety:</strong> Structured, unstructured, or semi-structured data. Complex data types might require more resources.</p></li><li><p><strong>Data Velocity:</strong> How quickly your data is generated and needs to be processed. Real-time data streams require different sizing considerations than batch data processing.</p></li><li><p><strong>Processing Needs:</strong> Are you performing simple data warehousing or complex analytics? Complex workloads require more powerful machines.</p></li><li><p><strong>Budgetary Constraints:</strong> The cost of storage, compute resources, and personnel can impact your ideal size.</p></li></ul><p>We'll explore popular platforms like AWS EMR, Redshift, Kafka, and others to estimate sizing needs.</p><hr><h3>Big Data Sizing Formulas: A Breakdown</h3><ol><li><p><strong>Data Lake Sizing:</strong></p><p><em>Total Storage</em> = Volume of Real-time Data + Volume of Batch Data<em>Total Storage after Retention</em> = Total Storage * (1 - Retention Percentage)</p><p>Example: Total Storage = 20 TB (Real-time) + 100 TB (Batch) = 120 TB, Total Storage after Retention = 96 TB (20% retention)</p></li><li><p><strong>Cluster Sizing for Data Processing (EMR):</strong></p><p><em>Instances</em> = ceil(total_data_size / (instance_capacity <em> parallelism))</em></p><p><em>Example: Instances = ceil(150 TB / (64 GB </em> 100)) = 24</p></li><li><p><strong>Memory Allocation (EMR):</strong></p><p><em>Total Memory</em> = Instances <em> Memory per Instance</em></p><p><em>Example: Total Memory = 24 Instances </em> 32 GB = 768 GB</p></li><li><p><strong>Redshift Storage:</strong></p><p><em>Node Count</em> = ceil(total_batch_data_size / (node_capacity <em> distribution_key_count))</em></p><p><em>Example: Node Count = ceil(100 TB / (2 TB </em> 32)) = 2</p></li><li><p><strong>Amazon S3 Storage:</strong></p><p>N/A: S3 storage depends on the volume and nature of data. No specific formula applies.</p><p>Example: Volume of Batch Data stored in S3: 80 TB</p></li><li><p><strong>Memory for Real-Time Streams (Kafka):</strong></p><p>N/A: Kafka memory depends on configuration parameters. No specific formula applies.</p><p>Example: Kafka Cluster Configuration: 10 nodes</p></li><li><p><strong>Stream Processing Memory:</strong></p><p><em>Total Memory</em> = Stream Instances <em> Memory per Instance</em></p><p><em>Example: Total Memory = 15 Instances </em> 16 GB = 240 GB</p></li><li><p><strong>Machine Learning Model Serving:</strong></p><p><em>Serving Instances</em> = ceil(total_prediction_volume / (instance_capacity <em> parallelism))</em></p><p><em>Example: Serving Instances = ceil(5 TB / (8 GB </em> 50)) = 13</p></li><li><p><strong>Edge Computing:</strong></p><p>N/A: Edge device sizing depends on the specific solution and workload. No specific formula applies.</p><p>Example: Number of Edge Devices: 50</p></li><li><p><strong>Network Bandwidth:</strong></p></li></ol><ul><li><p>N/A: Network bandwidth depends on data volume and network topology. No specific formula applies.</p></li><li><p>Example: Required Bandwidth: 1 Gbps</p></li></ul><ol><li><p><strong>Continuous Monitoring &amp; Scaling:</strong></p></li></ol><ul><li><p>N/A: Scaling policies and monitoring tools are specific to cloud providers. No specific formulas apply.</p></li><li><p>Example: Auto-scaling based on CPU utilization (Example)</p></li></ul><p><strong>Important Notes:</strong></p><ul><li><p>These formulas provide estimations. Always monitor your actual resource usage and adjust accordingly.</p></li><li><p>Consider factors like data compression, replication, and peak loads when calculating storage and processing needs.</p></li><li><p>Security and compliance overhead can vary depending on chosen methods.</p></li></ul><hr><h3>Big Data Sizing Formulas: Decoded</h3><figure><img data-media-urn="urn:li:digitalmediaAsset:D5612AQHkt1-Q_PLTuw" src="https://media.licdn.com/dms/image/v2/D5612AQHkt1-Q_PLTuw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1709415300680?e=1773878400&amp;v=beta&amp;t=XfCdzCM8euOV2SGgu2vlEjShnAcYZAPs2xf_dy2KhMU"><figcaption>Formulas decoded</figcaption></figure><p><strong>Additional Considerations:</strong></p><ul><li><p><strong>Data Compression:</strong> Compression can significantly reduce storage requirements. Factor in estimated compression ratios when calculating storage needs.</p></li><li><p><strong>Peak Loads:</strong> Account for peak query loads and concurrent users for Redshift and stream processing.</p></li><li><p><strong>Instance Types:</strong> Choose instance types based on your workload. For compute-intensive tasks, prioritize CPU cores. For memory-intensive operations, consider instances with higher memory.</p></li><li><p><strong>Monitoring and Scaling:</strong> Continuously monitor resource utilization and adjust configurations as needed. Utilize cloud provider's scaling features for dynamic adjustments.</p></li></ul><hr><h3>Big Data Sizing Cheat Sheet: A Breakdown by Scale</h3><p>This cheat sheet provides estimated resource requirements for different Big Data scenarios, categorized by size. Remember, these are starting points, and real-world deployments require adjustments based on specific needs.</p><p><strong>1. Small/Medium Data (Up to 100 TB):</strong></p><ul><li><p><strong>Data Lake Sizing:</strong></p><p><strong>Storage:</strong> 10 TB - 50 TB (Example: Sensor data collection for a small manufacturing plant)</p><p><strong>Retention:</strong> Retain data for 3-6 months depending on regulations and analysis needs (Example: Retention = 1 - 0.5 = 50% of data stored after 6 months)</p></li><li><p><strong>Cluster Sizing (EMR):</strong></p><p><strong>Instances:</strong> Utilize cloud provider guides for cost-effective options (Example: Utilize pre-configured EMR clusters with 2-4 instances)</p></li></ul><p><strong>2. Large Data (100 TB - 1 PB):</strong></p><ul><li><p><strong>Data Lake Sizing:</strong></p><p><strong>Storage:</strong> 100 TB - 500 TB (Example: E-commerce website with customer data and purchase history)</p><p><strong>Retention:</strong> Retain data for 1-2 years depending on legal and business requirements (Example: Retention = 1 - 0.8 = 20% of data stored after 2 years)</p></li><li><p><strong>Cluster Sizing (EMR):</strong></p><p><strong>Instances:</strong> Utilize formulas for estimations (Example: 150 TB data / (64 GB capacity <em> 100 parallelism) = 24 instances)</em></p><p><strong><em>Memory:</em></strong><em> Total Memory = Instances </em> Memory per Instance (Example: 24 instances * 32 GB/instance = 768 GB)</p></li><li><p><strong>Storage (Redshift):</strong></p><p><strong>Node Count:</strong> Utilize formulas for estimations (Example: 100 TB data / (2 TB capacity * 32 keys) = 2 nodes)</p></li></ul><p><strong>3. Extra Large Data (Over 1 PB):</strong></p><ul><li><p><strong>Data Lake Sizing:</strong></p><p><strong>Storage:</strong> Over 1 PB (Example: Large social media platform with user data and activity logs)</p><p><strong>Retention:</strong> Implement data lifecycle management with tiered storage and archiving (Example: Archive older data to cheaper storage options)</p><p><strong>Compression:</strong> Utilize data compression techniques to optimize storage efficiency (Example: Reduce storage footprint by 50% with compression)</p></li><li><p><strong>Cluster Sizing (EMR):</strong></p><p><strong>Instances:</strong> Utilize cloud provider guides for large-scale deployments (Example: Utilize high-performance instances with fewer nodes but more compute power)</p></li><li><p><strong>Storage (Redshift):</strong></p><p><strong>Node Count:</strong> Scale out with additional nodes for optimal performance (Example: Add more nodes to handle larger data volumes and complex queries)</p></li><li><p><strong>Other Considerations:</strong></p><p><strong>Network Bandwidth:</strong> Network bandwidth becomes critical for data movement and processing (Example: Consider dedicated high-bandwidth connections)</p></li></ul><p><strong>Additional Notes:</strong></p><ul><li><p>These are estimations. Monitor resource usage and adjust accordingly.</p></li><li><p>Real-world scenarios may require additional considerations like data complexity and processing needs.</p></li><li><p>Explore cloud provider tools for automated scaling and cost optimization.</p></li><li><p>Security and compliance overhead will vary depending on chosen methods</p></li></ul><h3>General Considerations Across Sizes:</h3><ul><li><p><strong>Amazon S3 Storage:</strong> No formula applies, depends on data volume and nature.</p></li><li><p><strong>Real-Time Streams (Kafka):</strong> No formula for cluster sizing, depends on configuration. Memory allocation formula applies for stream processing frameworks.</p></li><li><p><strong>Machine Learning (Serving):</strong> Formula applies for serving cluster sizing.</p></li><li><p><strong>Edge Computing &amp; Network Bandwidth:</strong> No formulas, depend on specific solution and network topology.</p></li><li><p><strong>Continuous Monitoring &amp; Scaling:</strong> Cloud provider tools and configurations determine these aspects.</p></li><li><p><strong>Security &amp; Compliance:</strong> Overhead varies based on chosen methods.</p></li></ul><p><strong>Formulas (Approximations):</strong></p><ul><li><p><strong>EMR Cluster Size:</strong> Nodes ≈ Data Volume (GB) / Instance Storage (GB) * Replication Factor (<strong>Note:</strong> This is a simplified formula and doesn't account for processing power requirements)</p></li><li><p><strong>Redshift Storage:</strong> Storage Required (GB) ≈ Data Volume (GB) / Compression Ratio (<strong>Compression Ratio:</strong> Varies based on data type - estimate 2-5x compression)</p></li><li><p><strong>Network Bandwidth:</strong> Bandwidth (Mbps) ≈ Data Ingestion Rate (GB/s) * 8 (Converts data rate to Mbps)</p></li></ul><hr><h3>Additional Resources:</h3><ul><li><p><a href="https://aws.amazon.com/ec2/instance-types" target="_blank"><strong>Amazon Web Services (AWS)</strong> Choosing the Right AWS EC2 Instance Type for Your Needs</a></p></li><li><p><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html" target="_blank">A Beginner's Guide to Setting Up an EMR Cluster on AWS</a></p></li><li><p><a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-emr-hardware/capacity.html" target="_blank"><strong>EMR Cluster Sizing Guide</strong></a></p></li><li><p><a href="https://docs.aws.amazon.com/redshift/" target="_blank"><strong>Redshift Sizing Guide</strong></a></p></li><li><p><a href="https://aws.amazon.com/blogs/big-data/getting-started-with-aws-lake-formation/" target="_blank">Building a Scalable Data Lake on AWS</a></p></li></ul><p><strong>Github Repositories</strong></p><ul><li><p><a href="https://github.com/aws-samples/aws-big-data-blog" target="_blank"><em>aws-big-data-best-practices</em> by Amazon Web Services</a></p></li><li><p><a href="https://github.com/aws-samples/aws-big-data-blog" target="_blank"><em>aws-big-data-solutions</em>  by Amazon Web Services</a></p></li></ul><p><strong>2. Explore Additional Resources:</strong></p><p>Consider these for a broader perspective:</p><ul><li><p><strong>Article (Microsoft Azure):</strong> Guide to choosing a virtual machine size for Azure HDInsight clusters: <a href="https://learn.microsoft.com/en-us/azure/hdinsight/hdinsight-selecting-vm-size" target="_blank">https://learn.microsoft.com/en-us/azure/hdinsight/hdinsight-selecting-vm-size</a> </p></li><li><p><strong>Github Repository (Example):</strong> Big Data Architecture on AWS: <a href="https://github.com/AWS-Big-Data-Projects" target="_blank">https://github.com/AWS-Big-Data-Projects</a> (This is an example, explore repositories based on your specific needs</p></li><li><p><strong>AWS Cost Calculator: </strong><a href="https://calculator.aws/" target="_blank"><strong>https://calculator.aws/</strong></a> (Estimate costs based on your chosen configuration)</p></li></ul></div>
</body>
</html>