<html>
<head>
  <title>The Information System Collapse - Part 3: The Architecture That Survives</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaD5612AQFszpAiy6uSOg" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/information-system-collapse-part-3-architecture-survives-paddy-iyer-svzdc">The Information System Collapse - Part 3: The Architecture That Survives</a></h1>
    <p class="created">Created on 2025-11-13 02:03</p>
  <p class="published">Published on 2025-11-13 02:17</p>
  <div><h3>From Collapse to Construction: Building Event-Driven Systems That Actually Work</h3><hr><h3>Where We've Been</h3><p><strong>Part 1</strong> exposed the delay problem: Your traditional ERP→ETL→Warehouse→BI stack takes 3 days to answer simple questions. By the time you get the answer, the opportunity is gone.</p><p><strong>Part 2</strong> revealed the source: ERP's two ancestral curses (Customization Chaos + Thousand-Table Labyrinth) force data teams to excavate instead of analyze.</p><blockquote><p>Now comes the hard question: <strong>What do we build instead?</strong></p></blockquote><hr><h3>The Honest Starting Point</h3><p>Let's be clear about what we're NOT doing:</p><ul><li><p>❌ We're not "eliminating data warehouses" (too evangelical) </p></li><li><p>❌ We're not claiming "AI solves everything" (too magical) </p></li><li><p>❌ We're not proposing a rip-and-replace (too risky)</p></li></ul><p>What we ARE doing:</p><ul><li><p>✅ Building a parallel system that handles 80% of business questions faster </p></li><li><p>✅ Using proven technologies (Kafka, PostgreSQL, LLMs) in a new way </p></li><li><p>✅ Starting with one use case, proving value, then expanding </p></li><li><p>✅ Keeping your existing infrastructure running while we transition</p></li></ul><p>This is <strong>engineering</strong>, not revolution.</p><hr><h3>The Core Insight: Events, Not States</h3><p>The fundamental problem with ERP→Warehouse architecture: <strong>It stores STATES (final values) instead of EVENTS (what actually happened).</strong></p><p>When your ERP records a sale, it stores:</p><ul><li><p>Order ID: 12345</p></li><li><p>Customer ID: CUST_789</p></li><li><p>Amount: $500</p></li><li><p>Date: 2025-01-15</p></li></ul><p>What it DOESN'T store:</p><ul><li><p>Customer browsed 7 products before buying</p></li><li><p>Added item to cart, removed it, added it back</p></li><li><p>Hesitated at checkout for 3 minutes</p></li><li><p>Applied a discount code</p></li><li><p>Checkout page loaded slowly (4 seconds)</p></li></ul><p><strong>All that context is LOST.</strong></p><p>And that context is exactly what you need to answer "why did this happen?"</p><hr><h3>The New Architecture: Event-Driven Intelligence</h3><figure><img data-media-urn="urn:li:digitalmediaAsset:D5612AQGxTBm4ypzxRw" src="https://media.licdn.com/dms/image/v2/D5612AQGxTBm4ypzxRw/article-inline_image-shrink_1500_2232/B56Zp7Bf0zI8AU-/0/1763000581885?e=1773878400&amp;v=beta&amp;t=1HWkYOB8c98SBErMVVhnsABL-GIQ95oh2dTWWNtSYhc"><figcaption></figcaption></figure><p></p><p>Here's what we're building:</p><pre></pre><p>Let's break down each layer with <strong>actual technical details</strong>, not buzzwords.</p><hr><h3>Layer 1: Event Capture (The Foundation)</h3><h3>What Is An Event?</h3><p>An event is an immutable record of something that happened, with full context.</p><p><strong>Example Event Schema:</strong></p><pre></pre><h3>Key Properties of Events</h3><ol><li><p><strong>Immutable</strong> Once written, never changed. To correct an error, you write a new event (like accounting).</p></li><li><p><strong>Self-Contained</strong> Every event has all the context it needs. No need to join 12 tables to understand it.</p></li><li><p><strong>Causally Linked</strong> Every event knows what caused it and what it might trigger.</p></li><li><p><strong>Semantically Indexed</strong> Each event has a vector embedding that captures its meaning, enabling semantic search.</p></li></ol><hr><h3>Layer 2: Semantic Event Fabric (The Smart Storage)</h3><p>This is where we store and index events in a way that makes them <strong>queryable by meaning</strong>, not just by field names.</p><h3>Technical Components</h3><p><strong>Component 1: Event Store (PostgreSQL with TimescaleDB)</strong></p><p>Why PostgreSQL?</p><ul><li><p>Battle-tested reliability</p></li><li><p>Native JSON support for flexible event payloads</p></li><li><p>TimescaleDB extension for time-series optimization</p></li><li><p>pgvector extension for semantic search</p></li><li><p>ACID guarantees (unlike some NoSQL solutions)</p></li></ul><p><strong>Schema:</strong></p><pre></pre><p><strong>Component 2: Event Bus (Apache Kafka or Redpanda)</strong></p><p>Why Kafka/Redpanda?</p><ul><li><p>Handles millions of events per second</p></li><li><p>Guaranteed ordering within partitions</p></li><li><p>Replay capability (reprocess events if needed)</p></li><li><p>Multiple consumers can read same stream</p></li></ul><p><strong>Topic Structure:</strong></p><pre></pre><p><strong>Component 3: Stream Processors (Kafka Streams or Flink)</strong></p><p>Real-time enrichment and aggregation:</p><pre></pre><hr><h3>Layer 3: Domain Reasoning Nodes (The Intelligence)</h3><p>These are <strong>specialized query engines</strong> for specific business domains. Not generic AI—focused, deterministic processors with LLM-enhanced reasoning.</p><h3>Anatomy of a Domain Reasoning Node</h3><p><strong>Example: Customer Behavior Node</strong></p><pre></pre><h3>Why This Architecture Works</h3><p><strong>90% of queries are deterministic</strong> (fast SQL):</p><ul><li><p>"Show me conversion funnel for last week"</p></li><li><p>"What's the cart abandonment rate by device?"</p></li><li><p>"How many orders today?"</p></li></ul><p><strong>10% of queries need synthesis</strong> (SQL + LLM):</p><ul><li><p>"Why did mobile conversion drop yesterday?"</p></li><li><p>"Which customer segments are at risk?"</p></li><li><p>"What happened before high-value users churned?"</p></li></ul><p>The LLM is only used for:</p><ol><li><p>Understanding intent (parsing natural language)</p></li><li><p>Finding semantic patterns (when causality needed)</p></li><li><p>Synthesizing explanations (turning data into narrative)</p></li></ol><p><strong>The LLM never does math.</strong> All calculations are SQL.</p><hr><h3>Layer 4: Conversational Query Interface</h3><p>Users don't write SQL. They ask questions.</p><h3>How It Works</h3><p><strong>Input:</strong> Natural language query <strong>Output:</strong> Answer with evidence and confidence</p><p><strong>Example Interaction:</strong></p><pre></pre><h3>Technical Implementation</h3><pre></pre><hr><h3>Real-World Example: E-commerce Flow</h3><p>Let's trace a complete customer journey through the system.</p><h3>Events Generated</h3><pre></pre><h3>Query Examples</h3><p><strong>Query 1: Simple Aggregation (Deterministic)</strong></p><pre></pre><p><strong>Query 2: Funnel Analysis (Deterministic)</strong></p><pre></pre><p><strong>Query 3: Diagnostic (Hybrid - SQL + Vector Search + LLM)</strong></p><pre></pre><hr><h3>Infrastructure Requirements (Real Numbers)</h3><p>For a mid-sized e-commerce company (1M visitors/month, 50K orders/month):</p><h3>Event Volume</h3><ul><li><p>~50M events/month</p></li><li><p>~60 events/second average</p></li><li><p>~300 events/second peak</p></li></ul><h3>Infrastructure</h3><p><strong>Event Store (PostgreSQL + TimescaleDB):</strong></p><ul><li><p>3-node cluster (primary + 2 replicas)</p></li><li><p>16 vCPU, 64GB RAM per node</p></li><li><p>2TB SSD storage (with compression)</p></li><li><p>Cost: ~$2,000/month (AWS RDS equivalent)</p></li></ul><p><strong>Event Bus (Kafka or Redpanda):</strong></p><ul><li><p>3-node cluster</p></li><li><p>8 vCPU, 32GB RAM per node</p></li><li><p>1TB SSD per node</p></li><li><p>Cost: ~$1,500/month</p></li></ul><p><strong>Domain Reasoning Nodes:</strong></p><ul><li><p>4 nodes (customer, revenue, inventory, product)</p></li><li><p>8 vCPU, 32GB RAM per node</p></li><li><p>Cost: ~$1,200/month</p></li></ul><p><strong>LLM API (Claude Sonnet):</strong></p><ul><li><p>~10K queries/month</p></li><li><p>~80% deterministic (no LLM needed)</p></li><li><p>~2K queries need LLM synthesis</p></li><li><p>Cost: ~$500/month</p></li></ul><p><strong>Vector Embeddings (OpenAI):</strong></p><ul><li><p>Batch process 50M events/month</p></li><li><p>Cost: ~$400/month</p></li></ul><p><strong>Total Infrastructure: ~$5,600/month</strong></p><h3>Compare to Traditional Stack</h3><p>Traditional (Snowflake + dbt + Fivetran + Looker):</p><ul><li><p>Snowflake: $3,000/month</p></li><li><p>Fivetran: $1,500/month</p></li><li><p>dbt Cloud: $500/month</p></li><li><p>Looker: $1,000/month</p></li><li><p>Data team time: 60% on maintenance</p></li><li><p><strong>Total: $6,000/month + massive time waste</strong></p></li></ul><p><strong>Event-driven stack: $5,600/month + 10% time on maintenance</strong></p><hr><h3>Migration Strategy: 90-Day Proof of Concept</h3><h3>Week 1-2: Event Capture POC</h3><p><strong>Goal:</strong> Prove we can capture events from existing systems</p><p><strong>Tasks:</strong></p><ol><li><p>Set up Kafka cluster</p></li><li><p>Write event producers for 1 source system (e.g., e-commerce platform)</p></li><li><p>Capture 1 event type (e.g., order_completed)</p></li><li><p>Store in PostgreSQL</p></li></ol><p><strong>Success metric:</strong> 100K events captured and stored</p><h3>Week 3-4: Build First Domain Node</h3><p><strong>Goal:</strong> Answer 1 business question faster than current system</p><p><strong>Tasks:</strong></p><ol><li><p>Build Revenue Node</p></li><li><p>Implement deterministic queries (simple aggregations)</p></li><li><p>Compare results with existing warehouse</p></li></ol><p><strong>Success metric:</strong> "Daily revenue by product" query: 0.5s (vs 3 minutes in warehouse)</p><h3>Week 5-6: Add Vector Search + Semantic Layer</h3><p><strong>Goal:</strong> Enable semantic queries</p><p><strong>Tasks:</strong></p><ol><li><p>Generate embeddings for all events</p></li><li><p>Implement vector similarity search</p></li><li><p>Test semantic queries</p></li></ol><p><strong>Success metric:</strong> "Find orders similar to this one" works accurately</p><h3>Week 7-8: Conversational Interface</h3><p><strong>Goal:</strong> Natural language queries work</p><p><strong>Tasks:</strong></p><ol><li><p>Integrate Claude API</p></li><li><p>Build intent parser</p></li><li><p>Implement answer synthesis</p></li><li><p>Test with 20 real business questions</p></li></ol><p><strong>Success metric:</strong> 90%+ accuracy on test questions</p><h3>Week 9-10: Hybrid Query Execution</h3><p><strong>Goal:</strong> Handle complex "why" questions</p><p><strong>Tasks:</strong></p><ol><li><p>Implement causal chain analysis</p></li><li><p>Build cross-domain query capability</p></li><li><p>Test diagnostic queries</p></li></ol><p><strong>Success metric:</strong> "Why did conversion drop?" answered in &lt;10 seconds with causality</p><h3>Week 11-12: Pilot with Real Users</h3><p><strong>Goal:</strong> Prove business value</p><p><strong>Tasks:</strong></p><ol><li><p>Give access to 5 business users</p></li><li><p>Track: queries asked, time saved, satisfaction</p></li><li><p>Compare accuracy vs traditional dashboards</p></li></ol><p><strong>Success metrics:</strong></p><ul><li><p>50+ queries asked</p></li><li><p>80%+ user satisfaction</p></li><li><p>100x faster than traditional method</p></li><li><p>Demonstrate ROI</p></li></ul><hr><h3>What Makes This Different</h3><h3>Not Data Warehouse 2.0</h3><p>This isn't "a faster warehouse." It's a different paradigm:</p><ul><li><p><strong>Old:</strong> Store aggregated states, visualize, human interprets </p></li><li><p><strong>New:</strong> Store events, synthesize answers, explain causality</p></li></ul><h3>Not "AI Does Everything"</h3><ul><li><p>90% is deterministic SQL (fast, accurate, cheap) 10% uses LLM (for understanding intent and synthesis)</p></li><li><p>The intelligence is in the architecture, not just the AI.</p></li></ul><h3>Not Rip-and-Replace</h3><p>Your existing warehouse keeps running. New system handles new use cases. Migrate gradually as you prove value.</p><h3>Not Vendor Lock-In</h3><p>Built on open technologies:</p><ul><li><p>PostgreSQL (open source)</p></li><li><p>Kafka (open source)</p></li><li><p>DuckDB (open source)</p></li><li><p>Claude API (<a href="http://anthropic.com" target="_blank">anthropic.com</a>, but swappable)</p></li></ul><p>You own the infrastructure.</p><hr><h3>The Path Forward</h3><ul><li><p><strong>Months 1-3:</strong> POC (prove it works) </p></li><li><p><strong>Months 4-6:</strong> Pilot (10-20 users, 20% of queries) </p></li><li><p><strong>Months 7-12:</strong> Scale (50% of queries) </p></li><li><p><strong>Year 2:</strong> Primary system (80% of queries) </p></li><li><p><strong>Year 3:</strong> Warehouse becomes archive only</p></li></ul><p><strong>You don't rip out the old system.</strong> <strong>You build the new one alongside it.</strong> <strong>You prove value before betting the company.</strong></p></div>
</body>
</html>